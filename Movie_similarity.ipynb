{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)                # with the seed reset the same set will appear everytime it is invoked else diferent set of inputs appear\n",
    "movies_df = pd.read_csv('movies.csv')                                                               # Reading IMDb and Wikipedia movie data\n",
    "print(\"Number of movies loaded: %s \" % (len(movies_df)))\n",
    "movies_df['plot'] = movies_df['wiki_plot'].astype(str) + \"\\n\" + movies_df['imdb_plot'].astype(str)  # Combine wiki_plot and imdb_plot into a single column\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing Tokenization and Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):                                           # Define a function to perform both stemming and tokenization\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = [y for text in nltk.sent_tokenize(text) for y in nltk.word_tokenize(text)]\n",
    "    \n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "    print(filtered_tokens)\n",
    "    # Stem the filtered_tokens\n",
    "    stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return stems\n",
    "\n",
    "words_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\n",
    "print(words_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF Vectorizer** \n",
    "1. Creating TfidfVectorizer recognizing words that are unique and that are important to any document\n",
    "2. Fitting the text into the Tfidf and producing numeric form of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.1, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"plot\"]])         #create a vector representation of the plot summaries\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing KMeans and creating clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5)            # Create a KMeans object with 5 clusters and save as km\n",
    "km.fit(tfidf_matrix)                 # Fit the k-means object with tfidf_matrix\n",
    "clusters = km.labels_.tolist()\n",
    "movies_df[\"cluster\"] = clusters      # Create a column cluster to denote the generated cluster for each movie\n",
    "movies_df['cluster'].value_counts()  # Display number of films per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating similarity distance, merging the matrix and Plotting dendrogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_distance = 1 - cosine_similarity(tfidf_matrix)     # Calculate the similarity distance\n",
    "mergings = linkage(similarity_distance, method='complete')    # Create mergings matrix \n",
    "dendrogram_ = dendrogram(mergings,\n",
    "               labels=[x for x in movies_df[\"title\"]],\n",
    "               leaf_rotation=90,\n",
    "               leaf_font_size=16,\n",
    ")                                                             # Plot the dendrogram, using title as label column\n",
    "fig = plt.gcf()                                               # Adjust the plot\n",
    "_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\n",
    "fig.set_size_inches(108, 21)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
